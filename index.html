<!-- <!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Report</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&family=Playfair+Display:wght@400;700&display=swap" rel="stylesheet">
</head>
<body>
    <div class="progress-bar"></div>
    <div class="container">
        <header class="fade-in">
            <div class="header-decoration"></div>
            <h1>Research Report Title</h1>
            <p class="author"><i class="fas fa-user"></i> Author Name</p>
            <p class="institution"><i class="fas fa-university"></i> Institution Name</p>
            <div class="header-decoration"></div>
        </header>

        <section id="abstract" class="slide-in">
            <div class="section-icon"><i class="fas fa-file-alt"></i></div>
            <h2>1. Abstract</h2>
            <p>Your abstract content goes here. This should be a brief summary of your entire research work, typically 150-250 words.</p>
        </section>

        <section id="toc" class="slide-in">
            <div class="section-icon"><i class="fas fa-list"></i></div>
            <h2>2. Table of Contents</h2>
            <nav>
                <ul>
                    <li><a href="#abstract"><i class="fas fa-chevron-right"></i> 1. Abstract</a></li>
                    <li><a href="#toc"><i class="fas fa-chevron-right"></i> 2. Table of Contents</a></li>
                    <li><a href="#introduction"><i class="fas fa-chevron-right"></i> 3. Introduction</a></li>
                    <li><a href="#literature"><i class="fas fa-chevron-right"></i> 4. Literature Review / Related Work</a></li>
                    <li><a href="#methodology"><i class="fas fa-chevron-right"></i> 5. Methodology & Implementation</a></li>
                    <li><a href="#results"><i class="fas fa-chevron-right"></i> 6. Results and Discussion</a></li>
                    <li><a href="#demo"><i class="fas fa-chevron-right"></i> 7. Demo of Simulation and Hardware</a></li>
                    <li><a href="#conclusion"><i class="fas fa-chevron-right"></i> 8. Conclusion and Future Work</a></li>
                    <li><a href="#references"><i class="fas fa-chevron-right"></i> 9. References</a></li>
                </ul>
            </nav>
        </section>

        <section id="introduction" class="slide-in">
            <div class="section-icon"><i class="fas fa-info-circle"></i></div>
            <h2>3. Introduction</h2>
            <p>Your introduction content goes here. Provide background information, research objectives, and the significance of your study.</p>
        </section>

        <section id="literature" class="slide-in">
            <div class="section-icon"><i class="fas fa-book"></i></div>
            <h2>4. Literature Review / Related Work</h2>
            <p>Present your literature review and related work here. Discuss previous research and establish the theoretical framework for your study.</p>
        </section>

        <section id="methodology" class="slide-in">
            <div class="section-icon"><i class="fas fa-cogs"></i></div>
            <h2>5. Methodology & Implementation</h2>
            <p>Describe your research methodology and implementation details here. Include any experimental setup, procedures, or technical specifications.</p>
        </section>

        <section id="results" class="slide-in">
            <div class="section-icon"><i class="fas fa-chart-bar"></i></div>
            <h2>6. Results and Discussion</h2>
            <p>Present your findings and discuss their implications. Include any relevant data, charts, or analysis.</p>
        </section>

        <section id="demo" class="slide-in">
            <div class="section-icon"><i class="fas fa-desktop"></i></div>
            <h2>7. Demo of Simulation and Hardware</h2>
            <p>Include demonstrations of your simulations and/or hardware implementations here. You can add images, videos, or interactive elements.</p>
        </section>

        <section id="conclusion" class="slide-in">
            <div class="section-icon"><i class="fas fa-flag-checkered"></i></div>
            <h2>8. Conclusion and Future Work</h2>
            <p>Summarize your key findings and discuss potential future research directions or improvements.</p>
        </section>

        <section id="references" class="slide-in">
            <div class="section-icon"><i class="fas fa-quote-right"></i></div>
            <h2>9. References</h2>
            <ul class="references-list">
                <li><i class="fas fa-bookmark"></i> Reference 1</li>
                <li><i class="fas fa-bookmark"></i> Reference 2</li>
                <li><i class="fas fa-bookmark"></i> Reference 3</li>
            </ul>
        </section>
    </div>

    <button id="scroll-top" class="scroll-top-btn" title="Go to top">
        <i class="fas fa-arrow-up"></i>
    </button>

    <script>
        // Scroll progress bar
        window.onscroll = function() {
            let winScroll = document.body.scrollTop || document.documentElement.scrollTop;
            let height = document.documentElement.scrollHeight - document.documentElement.clientHeight;
            let scrolled = (winScroll / height) * 100;
            document.querySelector(".progress-bar").style.width = scrolled + "%";
            
            // Show/hide scroll-to-top button
            let scrollBtn = document.querySelector(".scroll-top-btn");
            if (winScroll > 300) {
                scrollBtn.style.display = "block";
            } else {
                scrollBtn.style.display = "none";
            }
        };

        // Smooth scroll to top
        document.querySelector(".scroll-top-btn").addEventListener("click", function() {
            window.scrollTo({
                top: 0,
                behavior: "smooth"
            });
        });

        // Intersection Observer for slide-in animations
        const observer = new IntersectionObserver(
            (entries) => {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        entry.target.classList.add('visible');
                    }
                });
            },
            { threshold: 0.1 }
        );

        document.querySelectorAll('.slide-in').forEach(section => {
            observer.observe(section);
        });
    </script>
</body>
</html> -->


<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Report</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <link
        href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&family=Playfair+Display:wght@400;700&family=Fira+Code&display=swap"
        rel="stylesheet">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>

    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            },
            startup: {
                pageReady: () => {
                    return MathJax.startup.defaultPageReady().then(() => {
                        console.log('MathJax typesetting complete');
                    });
                }
            }
        };
    </script>
</head>

<body>
    <div class="progress-bar"></div>
    <div class="container">
        <header class="fade-in">
            <h1 style="font-size: 2.5em; font-weight: bold; text-align: center;">Tendon-driven Robotic hand for
                ASL-Translation</h1>

            <div style="text-align: center; margin-top: 1em;">
                <p class="author"><i class="fas fa-user"></i> A Vivek Varsil Raj - CB.SC.U4AIE23005</p>
                <p class="author"><i class="fas fa-user"></i> C V Shreyaas aditya - CB.SC.U4AIE23018</p>
                <p class="author"><i class="fas fa-user"></i> Roshan T - CB.SC.U4AIE23058</p>
                <p class="author"><i class="fas fa-user"></i> Suthekshan K - CB.SC.U4AIE23067</p>
                <p class="author"><i class="fas fa-user"></i> Syed Anees Ashraf - CB.SC.U4AIE23068</p>
            </div>

            <p class="institution" style="text-align: center; margin-top: 1em;">
                <i class="fas fa-university"></i> CSE - AI , Amrita Viswha Vishwa Vidyapeetham
            </p>

        </header>


        <section id="abstract" class="slide-in">
            <div class="section-icon"><i class="fas fa-file-alt"></i></div>
            <h2>Abstract</h2>
            <p>In an increasingly interconnected world, inclusivity and accessibility in communication remain critical
                goals of technological advancement. American Sign Language (ASL) serves as a primary mode of
                communication for millions within the deaf and hard-of-hearing communities. However, the language
                barrier often leads to isolation and reduced access to essential services for ASL users. This project
                proposes a comprehensive robotic system that bridges this gap through a tendon-driven robotic hand
                capable of both generating and interpreting ASL letters.</p>

            <p>In the Speech-to-ASL module, spoken input is captured and denoised using Fast Fourier Transform (FFT)
                techniques before being processed by OpenAI's Whisper AI model for accurate speech-to-text conversion.
                Each letter of the recognized text is then mapped to its corresponding ASL sign, with the robotic hand's
                servo motors positioned through forward kinematics to accurately replicate the gestures. The Text-to-ASL
                feature operates similarly, directly converting typed input into a sequence of ASL gestures by computing
                the servo positions for each letter.</p>

            <p>Additionally, the Hand Mimicry function uses real-time camera input to detect and recognize ASL gestures
                performed by a human, which are then mirrored by the robotic hand to create an interactive and
                educational communication tool.</p>

            <p>By combining advanced audio processing, machine learning, and robotics, this project aims to break down
                communication barriers and promote accessibility, demonstrating the potential of assistive technology in
                fostering human connection through sign language.</p>
        </section>

        <section id="toc" class="slide-in">
            <div class="section-icon"><i class="fas fa-list"></i></div>
            <h2>2. Table of Contents</h2>
            <nav>
                <ul>
                    <li><a href="#introduction"><i class="fas fa-chevron-right"></i> 1. Introduction</a></li>
                    <li><a href="#literature"><i class="fas fa-chevron-right"></i> 2. Literature Review / Related
                            Work</a></li>
                    <li><a href="#methodology"><i class="fas fa-chevron-right"></i> 3. Methodology & Implementation</a>
                    </li>
                    <li><a href="#results"><i class="fas fa-chevron-right"></i> 4. Results and Discussion</a></li>
                    <li><a href="#demo"><i class="fas fa-chevron-right"></i> 5. Demo of Simulation and Hardware</a></li>
                    <li><a href="#conclusion"><i class="fas fa-chevron-right"></i> 6. Conclusion and Future Work</a>
                    </li>
                    <li><a href="#references"><i class="fas fa-chevron-right"></i> 7. References</a></li>
                </ul>
            </nav>
        </section>

        <section id="introduction" class="slide-in">
            <div class="section-icon"><i class="fas fa-info-circle"></i></div>
            <h2>3. Introduction</h2>
            <p>Communication is a basic need in everyday life. But people who use sign language often face problems when
                others don’t understand it. Our project aims to solve this by building a robotic hand that can show
                American Sign Language (ASL) letters when someone speaks.</p>
            <p>Communication is fundamental to human interaction, yet millions of individuals in the Deaf and Hard of
                Hearing (DHH) community face barriers due to the lack of real-time, inclusive translation tools.
                American Sign Language (ASL), a primary mode of communication among DHH individuals, is not universally
                understood by the general population, leading to communication gaps in everyday interactions. Our
                project aims to address this issue by developing an intelligent, tendon-driven robotic hand capable of
                translating spoken English into ASL alphabet gestures.</p>
            <p>By integrating advancements in speech recognition, gesture synthesis, and robotics, the proposed system
                enables a seamless interaction bridge between hearing and non-hearing communities. The system utilizes
                speech-to-text processing (via Whisper model), inverse kinematics for joint actuation, and optimization
                algorithms to precisely position the robotic hand's fingers using tendon-driven mechanisms. Through this
                multi-disciplinary approach, the robotic hand translates spoken input into ASL letters in real time.</p>

            <p>These motions are guided by a combination of forward kinematics and L-BFGS-B-based optimization, ensuring
                high accuracy in gesture replication.</p>
            <blockquote>
                <b><big> The system architecture comprises four core features:</b></big>
                <br>
                <br>
                <ol>
                    <li>ASL Alphabet Display: A user can input a character or word, and the hand will animate the
                        corresponding ASL letter using predefined tendon configurations.</li>

                    <li>Speech to Text to ASL: Using Whisper for high-accuracy transcription, coupled with FFT-based
                        voice activity detection, the system captures live speech, processes it into text, and
                        decomposes it into ASL alphabet animations.</li>

                    <li>Gesture Mimicry: With MediaPipe’s real-time hand landmark detection, the system extracts 3D
                        joint angles from a user’s hand shown in front of a webcam and replicates the gesture using
                        inverse kinematics on the robot.</li>

                    <li>ASL Recognition via Machine Learning: A Random Forest classifier is trained to recognize ASL
                        letters from joint angles extracted from MediaPipe, allowing the robot to interpret what letter
                        the user is signing.</li>
                </ol>
            </blockquote>
        </section>

        <section id="literature" class="slide-in">
            <div class="section-icon"><i class="fas fa-book"></i></div>
            <h2>4. Literature Review / Related Work</h2>
            <table border="1" cellpadding="10" cellspacing="0">
                <thead>
                    <tr>
                        <th>Title</th>
                        <th>Technique</th>
                        <th>Key Idea (in simple words)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Joint-Space Torque and Stiffness Control (IROS – Abdallah et al.)</td>
                        <td>Uses joint-space control instead of tendon-space, and a tension algorithm that avoids
                            complex calculations</td>
                        <td>This method gives faster and better finger movements by directly controlling the joints. It
                            also keeps tendon forces within safe limits without using heavy math.</td>
                    </tr>
                    <tr>
                        <td>Calibration of Human Hand using CyberGlove (ASME IMECE 2000 – Griffin et al.)</td>
                        <td>Calibrates human hand using glove and maps it to robot hand using object-based control</td>
                        <td>This helps match a human hand's actions with a robot hand using simple tools, so the robot
                            hand copies the user's pinch and grip more accurately.</td>
                    </tr>
                    <tr>
                        <td>System Identification with L-BFGS-B (IEEE Transactions – Bemporad, 2025)</td>
                        <td>Used L-BFGS-B optimization with group sparsity for better model learning</td>
                        <td>This improves the robot's movement accuracy by finding good models with fewer calculations.
                            It is useful for systems like ours that rely on precise motion.</td>
                    </tr>
                    <tr>
                        <td>Tendon-Driven Robotic Hand for Tactile Sign Language (Thesis – Johnson, 2021)</td>
                        <td>Used cloud-based ASL system with 17-DOA robotic hand</td>
                        <td>This project helped Deaf-Blind users by converting ASL into hand movements using motors. It
                            showed great accuracy and helped people understand words by touch.</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <section id="methodology" class="slide-in">
            <div class="section-icon"><i class="fas fa-cogs"></i></div>
            <h2>Methodology</h2>
            <center>
                <h2>Speech To Text (VAD)</h2>
            </center>

            <p>In this project, we focus on identifying speech presence within audio signals using signal processing
                techniques. The methodology involves analyzing short chunks of audio and computing energy specifically
                within the speech-relevant frequency band (85 Hz to 3800 Hz).</p>

            <h3>1. Preprocessing and Framing</h3>
            <p>The input audio is divided into small frames or chunks of fixed length, for example, 1024 samples per
                frame. Each frame is processed individually.</p>

            <h3>2. Apply Hanning Window</h3>
            <p>To reduce spectral leakage caused by abrupt edges in audio chunks, we apply a Hanning window:</p>

            <p>
                \[
                w(n) = 0.5 \left(1 - \cos\left(\frac{2\pi n}{N - 1}\right)\right)
                \]
            </p>

            <p>Here, \( n \in [0, N-1] \) is the sample index within the chunk, and \( N \) is the number of samples in
                the chunk. This window smoothly tapers the beginning and end of the chunk to zero, minimizing
                discontinuities and improving the accuracy of the frequency domain representation.</p>

            <h3>3. Windowed Signal</h3>
            <p>Each audio frame \( x(n) \) is multiplied element-wise with the window function:</p>

            <p>
                \[
                x_{\text{windowed}}(n) = x(n) \cdot w(n)
                \]
            </p>

            <h3>4. Fast Fourier Transform (FFT)</h3>
            <p>We then convert the windowed time-domain signal to the frequency domain using the Discrete Fourier
                Transform, implemented efficiently via FFT:</p>

            <p>
                \[
                X(f) = \sum_{n=0}^{N-1} x(n) \cdot e^{-j 2\pi fn / N}
                \]
            </p>

            <p>This provides a frequency spectrum where each value \( X(f) \) represents the amplitude and phase of a
                frequency component \( f \).</p>

            <h3>5. Compute Magnitude Spectrum</h3>
            <p>The magnitude of each frequency component is calculated as:</p>

            <p>
                \[
                |X(f)| = \sqrt{\text{Re}(X(f))^2 + \text{Im}(X(f))^2}
                \]
            </p>

            <p>This removes the phase component, leaving only the strength of the frequency components.</p>

            <h3>6. Sum Energy in the Speech Band (85 Hz – 3800 Hz)</h3>
            <p>We compute the total energy in the range of frequencies most relevant to human speech:</p>

            <p>
                \[
                E_{\text{speech}} = \sum_{f=85}^{3800} |X(f)|
                \]
            </p>

            <p>This step helps isolate the energy related to voiced speech, filtering out irrelevant background noise
                such as very low rumble or high-frequency hiss.</p>

            <h3>7. Voice Activity Detection (VAD)</h3>
            <p>The computed energy is compared to a threshold to decide if speech is present in the chunk:</p>

            <p>
                \[
                \text{If } E_{\text{speech}} > \text{Threshold, then Speech is Active}
                \]
            </p>

            <h3>Why the Range 85-3800 Hz?</h3>
            <p>This frequency band is based on the typical spectrum of human speech. Most vocal energy (especially
                vowels and voiced consonants) lies in this range. Frequencies below 85 Hz usually contain environmental
                noise, while those above 3800 Hz mostly include sibilance and high-frequency noise.</p>


            <div class="hand-control-process"
                style="margin: 20px 0; padding: 10px; border: 1px solid #ffffff; border-radius: 5px; background-color: #ffffff;">
                <center>
                    <h2>Hand
                        Control Process</h2>
                </center>
                <ol
                    style="font-family: 'Roboto', sans-serif; font-size: 1em; color: #555; line-height: 1.6; padding-left: 20px;">
                    <li style="margin-bottom: 5px;">User speaks a word or sentence</li>
                    <li style="margin-bottom: 5px;">Speech is converted to text</li>
                    <li style="margin-bottom: 5px;">Each letter is converted into target finger positions</li>
                    <li style="margin-bottom: 5px;">IK calculates pull → servo angle</li>
                    <li style="margin-bottom: 5px;">Servo angle sent to Arduino</li>
                    <li style="margin-bottom: 5px;">Fingers move and show the ASL letter</li>
                </ol>
            </div>



            <h3>3.1 Forward Kinematics</h3>
            <p>
                Forward kinematics means calculating the position of the fingertip based on how much each joint is bent
                (joint angles).
                We use rotation matrices to model how the finger segments move when each joint rotates.
            </p>
            <p>
                Suppose we have 3 joints (like in a finger), and each joint can rotate around the X-axis.
                We use a rotation matrix to rotate each link. The rotation matrix for rotation about the X-axis is:
            </p>
            <p>
                \[
                R_x(\theta) =
                \begin{bmatrix}
                1 & 0 & 0 \\
                0 & \cos\theta & -\sin\theta \\
                0 & \sin\theta & \cos\theta
                \end{bmatrix}
                \]
            </p>
            <p>
                The position of each joint is calculated step-by-step using this formula:
            </p>
            <p>
                \[
                P_i = P_{i-1} + R_1 R_2 \dots R_i \cdot (L_i \cdot \text{direction})
                \]
            </p>
            <ul>
                <li>\( P_i \): position of the i-th joint</li>
                <li>\( R_i \): rotation matrix of the i-th joint</li>
                <li>\( L_i \): length of the i-th finger segment</li>
                <li>Direction: usually along the Y-axis \([0, 1, 0]\)</li>
            </ul>

            <h3>3.2 Inverse Kinematics</h3>
            <p>
                Inverse Kinematics is the opposite. Here, we start with a desired fingertip position (target),
                and we try to find the joint angles or tendon pull value that can move the finger to that position.
            </p>
            <p>
                Since our fingers are controlled by tendons, we use a single pull value \( p \) (in cm) to control all
                joints in that finger.
                Each joint rotates depending on this pull value, using a ratio.
            </p>
            <p>
                Joint angle is calculated from pull using this formula:
            </p>
            <p>
                \[
                \theta_i = \left( \frac{p}{\text{max_pull}} \right) \cdot r_i \cdot \theta_{i,\text{max}}
                \]
            </p>
            <ul>
                <li>\( p \): current tendon pull (in cm)</li>
                <li>\( \text{max_pull} \): maximum allowed pull (e.g., 2.5 cm)</li>
                <li>\( r_i \): ratio for each joint (e.g., 1.0, 0.7, 0.5)</li>
                <li>\( \theta_{i,\text{max}} \): maximum angle that the joint can reach</li>
            </ul>

            <h3>3.3 Optimization to Solve Inverse Kinematics</h3>
            <p>
                To find the correct pull \( p \), we use an optimization method called L-BFGS-B.
                It tries different pull values and checks how close the fingertip reaches the target position using
                forward kinematics.
            </p>
            <p>
                The objective (or cost) function to minimize is:
            </p>
            <p>
                $$
                \min_P \left\| \vec{p}_{\text{tip}}(P) - \vec{p}_{\text{target}} \right\|^2
                $$
            </p>

            <p>
                Where:<br>
                \( \vec{p}_{\text{tip}}(P) \): Fingertip position as a function of pull \( P \)<br>
                \( \vec{p}_{\text{target}} \): Desired fingertip position<br>
            </p>
            <p>
                This function calculates the square of the distance between the current fingertip position and the
                target.
                The optimizer finds the best \( p \) that gives the smallest distance.
            </p>
            <p>
                In Python, we used this line of code to find the optimal \( p \):
            </p>
            <pre><code>
result = minimize(objective, x0=[0.5 * max_pull], bounds=[(0.0, max_pull)], method='L-BFGS-B')
</code></pre>

            <h3>3.4 Optimization using L-BFGS-B</h3>
            <p> In our project, we need to find the right tendon pull (p) so that the robotic fingertip reaches a target
                position in 3D space. To do this, we use an optimization method called <b>L-BFGS-B</b>. </p>
            <h4>What is BFGS?</h4>
            <p> BFGS stands for <b>Broyden–Fletcher–Goldfarb–Shanno</b>, which is a popular optimization algorithm used
                to find the minimum of a function. It is especially useful when we don’t know the exact formula of the
                function, but we can still calculate its value and gradient (how steep it is). </p>
            <p> BFGS is a <b>quasi-Newton method</b>, which means it tries to guess how the function curves (second
                derivatives) without computing them exactly. It does this by building an approximation of the Hessian
                matrix (which represents curvature). </p>
            <h4>Why use L-BFGS-B?</h4>
            <p> In our case, we only have <b>one variable</b> per finger — the pull amount \( p \). But:
            <ul>
                <li>We want the pull to stay within a certain range (e.g., 0 cm to 2.5 cm).</li>
                <li>We want fast and memory-efficient optimization.</li>
            </ul> That's why we use <b>L-BFGS-B</b>, which stands for: <ul>
                <li><b>L</b>: Limited-memory (uses less RAM)</li>
                <li><b>BFGS</b>: Based on the BFGS method</li>
                <li><b>B</b>: Handles Bound constraints (like min and max pull values)</li>
            </ul>
            </p>
            <h4>How does it work?</h4>
            <p> The optimizer tries different values of \( p \), each time calculating the error between the fingertip's
                current position and the desired target position: </p>
            <p> \( f(p) = \left\| \text{tip\_position}(p) - \text{target\_position} \right\|^2 \) </p>
            <p> It also estimates how much the error changes if we increase or decrease \( p \). This is called the
                <b>gradient</b>:
            </p>
            <p> \( f'(p) = \frac{df}{dp} \) </p>
            <p> Using this gradient and its own memory of past steps, L-BFGS-B decides the next value of \( p \) to try.
                It continues this process until the error is very small or doesn't improve much anymore. </p>
            <h4>Why is this useful in our project?</h4>
            <p> Since we don’t know the exact function that maps pull to fingertip position (it depends on rotation and
                geometry), L-BFGS-B helps us <b>find the best pull amount without solving the math manually</b>. </p>
            <p> We used this method in Python like this: </p>
            <pre><code> result = minimize(objective, x0=[0.5 * max_pull], bounds=[(0.0, max_pull)], method='L-BFGS-B') </code></pre>
            <p> This line tries to find the pull value that makes the fingertip match the target position, while staying
                between 0 and max_pull. </p>




            <h3>3.5 Pull to Servo Conversion</h3>
            <p>
                Once we find the correct pull value, we need to convert it to a servo motor angle.
                Each servo pulls a string around a horn with a known radius. The formula to convert pull to servo angle
                is:
            </p>
            <p>
                \[
                \text{Servo\_Angle} = \left( \frac{p}{2\pi r} \right) \cdot 360^\circ
                \]
            </p>
            <ul>
                <li>\( p \): tendon pull in cm</li>
                <li>\( r \): radius of the servo horn (1.4 cm in our setup)</li>
            </ul>
            <p>
                We send this servo angle to an Arduino, which moves the corresponding servo motor.
                This pulls the tendon, and the finger bends to the right position.
            </p>



            <center><img src="k.png" alt="" style="width: 400px; height: 300px;"></center>
            <figcaption style="font-size: 14px; color: #555; margin-top: 5px; text-align: center;">
                Figure: Visualization of Forward and Inverse Kinematics in the Robotic Hand
            </figcaption>

            <center>
                <h2>Simulation Environment</h2>
            </center>

            <p>
                The simulation of the tendon-driven robotic hand was developed using Python with the help of the
                <code>matplotlib</code> and <code>numpy</code> libraries. It provides a fully interactive 3D
                visualization that models both the forward and inverse kinematics of a robotic finger. Each finger is
                simulated independently to reflect realistic motion based on tendon pulls.
            </p>

            <h4>1. Finger Structure and Coordinate System</h4>
            <p>
                Each finger is modeled as a 3-link serial kinematic chain, where each link represents a bone segment
                (proximal, middle, distal phalanges). The motion of each joint is simulated as rotation around the
                X-axis, and each joint angle is derived from the tendon pull using a linear mapping:
            </p>

            <p>
                \[
                \theta_i = \alpha \cdot r_i \cdot \theta_{i,\text{max}}, \quad \alpha =
                \min\left(\frac{P}{P_{\text{max}}}, 1.0\right)
                \]
            </p>

            <p>
                Here, \( \alpha \) is the normalized pull ratio, \( r_i \) is the pull-to-angle ratio for joint \( i \),
                and \( \theta_{i,\text{max}} \) is the maximum rotation angle in radians.
            </p>

            <h4>2. Visualization in 3D Space</h4>
            <p>
                The simulation uses a 3D coordinate space with the following layout:
            </p>
            <ul>
                <li><strong>X-axis</strong>: Horizontal spacing between fingers</li>
                <li><strong>Y-axis</strong>: Upward direction of finger bending</li>
                <li><strong>Z-axis</strong>: Depth, used for bending in the simulated plane</li>
            </ul>
            <br>
            <center><img src="sim.png" alt="" style="width: 500px; height: 400px;"></center>
            <figcaption style="font-size: 14px; color: #555; margin-top: 5px; text-align: center;">
                Figure:Simulation interface
            </figcaption>
            <br>
            <p>
                Each finger's base is placed along the X-axis with fixed spacing. Joint positions are computed
                sequentially using rotation matrices:
            </p>

            <p>
                \[
                \vec{p}_{i+1} = \vec{p}_i + R_1 R_2 \cdots R_i \cdot \vec{l}_i
                \]
            </p>

            <p>
                Where \( \vec{p}_i \) is the position of the current joint, \( R_i \) is the rotation matrix for the
                \(i\)-th joint, and \( \vec{l}_i \) is the local link vector along the Y-axis.
            </p>

            <h4>3. Interactive Controls</h4>
            <p>
                The simulation GUI includes the following interactive elements:
            </p>
            <ul>
                <li><strong>Sliders</strong> for each finger to manually adjust the tendon pull (0 to 2.5 cm).</li>
                <li><strong>Text boxes</strong> to enter a 3D target position and select a specific finger (0–4).</li>
                <li><strong>Solve button</strong> to run inverse kinematics and compute the required pull for the given
                    target.</li>
            </ul>

            <p>
                These elements enable both manual and automated control of finger movements, allowing users to visualize
                how different tendon pulls affect the joint angles and fingertip position.
            </p>

            <h4>4. Inverse Kinematics Solver</h4>
            <p>
                When a target position is entered, the simulation calls the inverse kinematics (IK) solver, which uses
                the L-BFGS-B optimization method to minimize the error between the simulated fingertip position and the
                desired target:
            </p>

            <p>
                \[
                \min_P \left\| \vec{p}_{\text{tip}}(P) - \vec{p}_{\text{target}} \right\|^2
                \]
            </p>

            <p>
                The solver returns the optimal tendon pull \( P \) and the resulting joint angles. This value is then
                applied to update the slider and finger position in the simulation.
            </p>

            <h4>
                The simulation dynamically displays:
            </h4>
            <ul>
                <li>The 3D finger segment positions as blue lines and joints.</li>
                <li>The fingertip position as a red dot.</li>
                <li>The target position (if any) as a green dot.</li>
                <li>Live joint angles and pull length for each finger.</li>
            </ul>

            <br>
            <h4>
                This simulation serves multiple purposes:
            </h4>
            <ul>
                <li>Validates the kinematic model before hardware implementation.</li>
                <li>Allows testing of inverse kinematics under various scenarios.</li>
                <li>Facilitates debugging and understanding of tendon-driven actuation.</li>
            </ul>

            <p>
                By integrating theory with visualization, the simulation bridges the gap between algorithm design and
                real-world deployment on robotic hardware.
            </p>



            <div class="system-architecture">
                <h3>System Architecture and Hardware Design</h3>
                <p>The robotic hand was engineered to imitate human finger movements using a tendon-driven actuation
                    mechanism. Each finger consists of three segments, with motion generated by pulling tendons routed
                    through these segments and attached to servo motors mounted at the base.</p>
                <ul>
                    <li>Servo horns with a radius of 1.4 cm convert rotational motion into linear tendon displacement.
                    </li>
                    <li>The tendons wrap around these horns, enabling finger bending or straightening as the servo
                        rotates.</li>
                </ul>
            </div>



        </section>
        <section id="hand-mimicry" class="slide-in">
            <div class="section-icon"><i class="fas fa-hand-paper"></i></div>
            <h2>Hand Mimicry System Methodology</h2>

            <p>The hand mimicry system enables real-time tracking of human hand gestures and replication by a robotic
                hand. This system combines computer vision, kinematic modeling, and servo control to achieve accurate
                gesture mirroring.</p>

            <h3>1 Hand Tracking with MediaPipe</h3>
            <p>The system uses Google's MediaPipe library for robust hand landmark detection:</p>
            <ul>
                <li><strong>Landmark Detection:</strong> MediaPipe identifies 21 key points on the hand (joints and
                    fingertips) with high accuracy</li>
                <li><strong>3D Position Estimation:</strong> Provides x, y, z coordinates for each landmark relative to
                    the camera frame</li>
                <li><strong>Real-time Processing:</strong> Optimized for live video feed with minimal latency (30+ FPS)
                </li>
            </ul>

            <h3>2 Joint Angle Calculation</h3>
            <p>For each detected hand, the system calculates joint angles using vector geometry:</p>
            <p>
                \[
                \theta = \cos^{-1}\left(\frac{\vec{v}_1 \cdot \vec{v}_2}{\|\vec{v}_1\|\|\vec{v}_2\|}\right)
                \]
            </p>
            <p>Where:</p>
            <ul>
                <li>\(\vec{v}_1\) and \(\vec{v}_2\) are vectors between consecutive joints</li>
                <li>\(\theta\) is the calculated bend angle (0°-180°)</li>
            </ul>

            <h3>3 Forward Kinematics Model</h3>
            <p>The system models each finger as a 3-link kinematic chain:</p>
            <ul>
                <li>Each finger segment has defined length based on human anatomy</li>
                <li>Joint rotations are computed using rotation matrices about the X-axis:
                    \[
                    R_x(\theta) = \begin{bmatrix}
                    1 & 0 & 0 \\
                    0 & \cos\theta & -\sin\theta \\
                    0 & \sin\theta & \cos\theta
                    \end{bmatrix}
                    \]
                </li>
                <li>Finger positions are calculated sequentially from base to tip</li>
            </ul>

            <h3>4 Inverse Kinematics for Robotic Control</h3>
            <p>The system uses L-BFGS-B optimization to determine tendon pull amounts:</p>
            <ol>
                <li>Define target fingertip position from human hand tracking</li>
                <li>Formulate optimization problem to minimize distance between current and target positions</li>
                <li>Use bounded optimization to keep pull values within physical limits (0-2.5 cm)</li>
                <li>Convert optimal pull amount to servo angles using:
                    \[
                    \text{Servo Angle} = \left(\frac{p}{2\pi r}\right) \cdot 360^\circ
                    \]
                    where \(p\) is tendon pull and \(r\) is servo horn radius (1.4 cm)
                </li>
            </ol>

            <h3>5 Real-time Control Pipeline</h3>
            <div class="hand-mimicry-process">
                <ol>
                    <li>Capture video frame from webcam</li>
                    <li>Detect hand landmarks using MediaPipe</li>
                    <li>Calculate joint angles for each finger segment</li>
                    <li>Compute target positions for robotic fingertips</li>
                    <li>Solve inverse kinematics for each finger</li>
                    <li>Convert pull values to servo angles</li>
                    <li>Send commands to Arduino via serial communication</li>
                    <li>Repeat at 10Hz update rate</li>
                </ol>
            </div>
            <br>
            <center>
                <img src="hand.png" alt="">
                <figcaption style="font-size: 14px; color: #555; margin-top: 5px;">Figure: MediaPipe Keypoints
                </figcaption>
            </center>
            <br>
            <h3>4.6 Special Handling for Thumb Rotation</h3>
            <p>The thumb requires additional processing due to its unique range of motion:</p>
            <ul>
                <li>Calculate thumb rotation angle based on wrist, thumb base, and index base positions</li>
                <li>Project vectors onto X-Z plane for robust angle calculation</li>
                <li>Map calculated angle to servo range (0°-180°)</li>
                <li>Apply special calibration adjustments for accurate positioning</li>
            </ul>

            <h3>4.7 System Integration</h3>
            <p>The complete system integrates multiple components:</p>
            <ul>
                <li><strong>Computer Vision:</strong> MediaPipe for hand tracking</li>
                <li><strong>Kinematic Modeling:</strong> Custom forward and inverse kinematics</li>
                <li><strong>Control System:</strong> Arduino with servo motors</li>
                <li><strong>Communication:</strong> Serial protocol for real-time commands</li>
            </ul>
        </section>
        <section id="workflow" class="slide-in">
            <center>
                <h2>Workflow</h2>
            </center>
            <center>
                <img src="DashBoardr.png" alt="" style="width: 100%; height: 400px;">
                <figcaption style="font-size: 14px; color: #555; margin-top: 5px;">Figure: Workflow</figcaption>
            </center>

        </section>
        <section id="results" class="slide-in">
            <div class="section-icon"><i class="fas fa-chart-bar"></i></div>
            <h2>6. Results and Discussion</h2>
            <center> <h2>Results</h2></center>
            <p>
                The tendon-driven robotic hand was evaluated across three key modules: Speech-to-ASL, Text-to-ASL, and
                Hand Mimicry.
                Each module demonstrated promising results in bridging communication gaps for the deaf and
                hard-of-hearing community.
            </p>
            <h3>6.1 Speech-to-ASL and Text-to-ASL Conversion</h3>
            <p>
                The integration of FFT-based speech preprocessing with OpenAI's Whisper AI model yielded accurate and
                noise-resilient
                speech-to-text conversion. By mapping each recognized character to its corresponding ASL representation,
                the robotic
                hand successfully formed ASL alphabets using servo-controlled tendons. The servo positioning was
                computed through
                optimized joint angles using forward kinematics and the L-BFGS-B minimization algorithm, achieving low
                positional
                error in fingertip placement. The robotic hand reproduced 13 ASL alphabets with high accuracy, enabling
                seamless
                translation from both voice and text inputs.
            </p>
            <ul>
                <li>Using FFT significantly improved transcription clarity in noisy environments.</li>
                <li>Threaded coordination between transcription and hand actuation ensured smooth, real-time
                    interaction.</li>
                <li>The system performed well under continuous speech, though pause detection required fine-tuning.</li>
            </ul>
            <br>
            <h3>5.2 Hand Mimicry Mode</h3>
            <p>
                This feature used a webcam to detect real-time ASL gestures performed by a human user. The MediaPipe
                hand landmark
                model identified joint positions, which were then used to calculate bend angles and reconstructed in the
                robotic
                hand using forward kinematics. The mirrored gestures closely matched human input, creating an
                interactive and
                educational tool for ASL learners and facilitators.
            </p>

            <ul>
                <li>The robotic hand successfully mimicked key static ASL gestures.</li>
                <li>Accuracy was slightly reduced for dynamic or complex gestures involving rapid motion due to
                    frame-rate and latency constraints.</li>
                <li>Real-time visualization helped validate the correctness of the mirrored signs.</li>
            </ul>
            <br>
            <h3>5.3 System-Wide Evaluation</h3>
            <ul>
                <li><strong>Gesture Reproduction Accuracy:</strong> ~70–75% across all alphabets.</li>
                <li><strong>Speech Recognition Accuracy (Whisper + FFT):</strong> ~92% in moderately noisy environments.
                </li>
                <li><strong>Mimicry Latency:</strong> 300–500 ms from camera detection to hand response.</li>
            </ul>
            <br>
            <center> <h2>Discussion</h2></center>
            <p>
                The results show that using a tendon-driven robotic hand for translating ASL is possible and works well.
                The method we used to calculate finger positions (joint angles) helped make the hand movements more accurate.
                This is important because tendon systems do not have sensors at each joint.
                Also, the design we used allows us to easily add more features in the future, like showing full words or sentences in ASL.
              </p>
              <p>
                However, recognizing and creating moving (dynamic) signs is still difficult. This is because of hardware speed limits
                and challenges in detecting the exact start and end of gestures. In the future, adding more joints (for better finger
                movement) and using touch sensors can make the hand more realistic and accurate.
              </p>
              <br>
        </section>

        <section id="demo" class="slide-in">
            <div class="section-icon"><i class="fas fa-desktop"></i></div>
            <h2>7. Demo of Simulation and Hardware</h2>
            <p>In this section, we will demonstrate the simulation and hardware of the robotic hand.</p>
            <center>
                <img src="2.jpg" alt="" style="width: 100%; height: 400px;">
                <figcaption style="font-size: 14px; color: #555; margin-top: 5px;">Figure: Hand Hardware</figcaption>
            </center>
            <div class="video-container" style="text-align: center; margin-top: 20px;">
                <video controls width="800" height="450">
                    <source src="3.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <p style="font-size: 14px; color: #555; margin-top: 5px;">Figure: Demonstration of Simulation</p>
            </div>
            <br><br>
            <div class="video-container" style="text-align: center; margin-top: 20px;">
                <video controls width="800" height="450">
                    <source src="2.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <p style="font-size: 14px; color: #555; margin-top: 5px;">Figure: Demonstration of Hardware (Alphabets)</p>
            </div>
            <br><br>
            <div class="video-container" style="text-align: center; margin-top: 20px;">
            <video controls width="800" height="450">
                <source src="1.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <p style="font-size: 14px; color: #555; margin-top: 5px;">Figure: Demonstration of Hardware (Mimicry)</p>
        </div>
        </section>

        <section id="conclusion" class="slide-in">
            <div class="section-icon"><i class="fas fa-flag-checkered"></i></div>
            <h2>8. Conclusion and Future Work</h2>

            <h3>Conclusion</h3>
            <p>This project successfully developed a 3D-printed robotic hand capable of translating speech, text, and
                visual ASL gestures into corresponding hand signs. By integrating servo motor control, forward
                kinematics, speech recognition, and computer vision, the system offers an effective tool for bridging
                communication gaps between hearing and deaf individuals. The results demonstrate the potential of
                combining robotics and AI to create accessible, real-time assistive communication technologies.
            </p>

            <h3>Future Work</h3>
            <p>While this project successfully demonstrates a functional prototype for translating speech, text, and
                gestures into American Sign Language (ASL) using a robotic hand, several improvements and expansions are
                planned for future development.</p>

            <h3>1.Voice Recognition Model</h3>

            <p> Currently, the system relies on OpenAI’s Whisper model for speech-to-text conversion. As a next step, we
                aim to develop a custom voice recognition system using deep learning neural networks. This will provide
                greater flexibility, offline capability, and customization for specific accents, dialects, or
                specialized vocabularies relevant to ASL users.</p>

            <h3> 2. Enhanced Finger Dexterity and Degrees of Freedom</h3>

            <p>In addition, the mechanical design of the robotic hand can be further enhanced by increasing the degrees
                of freedom (DOF) for each finger. This will allow for more natural, precise, and flexible finger
                movements, which not only improves the clarity of ASL gestures but also opens the possibility for
                expanding the system into a multi-purpose robotic hand capable of tasks such as object manipulation,
                grasping, and pick-and-place operations.</p>

            <h3> 3.EEG-Based Brain-Computer Interface (BCI) Integration</h3>

            <p>An ambitious and transformative extension of this project involves integrating EEG
                (Electroencephalography) brain signal processing. By capturing and interpreting neural activity related
                to specific thoughts or words, the system could allow users to translate thought patterns directly into
                ASL gestures via the robotic hand. This integration would enable a non-verbal, non-physical mode of
                communication, providing an invaluable tool for individuals with severe physical disabilities or
                speech impairments.</p>

        </section>

        <section id="references" class="slide-in">
            <div class="section-icon"><i class="fas fa-quote-right"></i></div>
            <h2>9. References</h2>
            <ul class="references-list">
                <li><i class="fas fa-bookmark"></i> Johnson, A. (2021). Robotic arm signing tactile sign language (t-SL) to aid deaf-blind communication (Master’s thesis, Northeastern University).</li>
                <li><i class="fas fa-bookmark"></i> Jalaja S, & Kiruthiga Chandra Shekar. (2022). "Robotic Arm for Sign Language Interpretation with Sentiment Analysis and Auto-Complete Text Features." International Journal of Engineering Research and Applications, 12(10), 92-99.</li>
                <li><i class="fas fa-bookmark"></i>  Kumar, S., & Gupta, A. (2023). "Gesture Generation by the Robotic Hand for Aiding Speech and Hearing-Impaired Persons Based on American Sign Language." SSRN Electronic Journal.</li>
                <li><i class="fas fa-bookmark"></i>  Chong, Y. S., & Lee, B. S. (2018). "American Sign Language Recognition Using Leap Motion Sensor and Deep Neural Network."</li>
                <li><i class="fas fa-bookmark"></i>  Maliki, R., Alhaidar, D., Attallah, K., Alsalem, S., Morris, M., & Tosunoglu, S. (2017). "Robotic Hands to Teach Sign Language."</li>

            </ul>
        </section>
    </div>

    <button id="scroll-top" class="scroll-top-btn" title="Go to top">
        <i class="fas fa-arrow-up"></i>
    </button>

    <script>
        // Scroll progress bar
        window.onscroll = function () {
            let winScroll = document.body.scrollTop || document.documentElement.scrollTop;
            let height = document.documentElement.scrollHeight - document.documentElement.clientHeight;
            let scrolled = (winScroll / height) * 100;
            document.querySelector(".progress-bar").style.width = scrolled + "%";

            // Show/hide scroll-to-top button
            let scrollBtn = document.querySelector(".scroll-top-btn");
            if (winScroll > 300) {
                scrollBtn.style.display = "block";
            } else {
                scrollBtn.style.display = "none";
            }
        };

        // Smooth scroll to top
        document.querySelector(".scroll-top-btn").addEventListener("click", function () {
            window.scrollTo({
                top: 0,
                behavior: "smooth"
            });
        });

        // Intersection Observer for slide-in animations
        const observer = new IntersectionObserver(
            (entries) => {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        entry.target.classList.add('visible');
                    }
                });
            },
            { threshold: 0.1 }
        );

        document.querySelectorAll('.slide-in').forEach(section => {
            observer.observe(section);
        });

        // Smooth scroll for navigation links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                document.querySelector(this.getAttribute('href')).scrollIntoView({
                    behavior: 'smooth'
                });
            });
        });
    </script>
</body>

</html>
